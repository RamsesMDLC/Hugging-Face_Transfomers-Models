{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEDGsZYU2eS32GZUKzU6YJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RamsesMDLC/Hugging-Face_Transfomers-Models/blob/main/Hugging_Face_TransformerModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#API key\n",
        "  #Provides a secure way to access stored secrets (like API tokens) within Google Colab.\n",
        "from google.colab import userdata\n",
        "  #Allows programmatic login to Hugging Face Hub.\n",
        "from huggingface_hub import login"
      ],
      "metadata": {
        "id": "2Q3UcGjdm6iQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "damsnDaul_Tw",
        "outputId": "5272a9e9-9cf3-4109-b749-27af0e361372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully logged in to Hugging Face!\n"
          ]
        }
      ],
      "source": [
        "# Securely get Hugging Face token and login\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "if hf_token:\n",
        "    login(hf_token)\n",
        "    print(\"Successfully logged in to Hugging Face!\")\n",
        "else:\n",
        "    print(\"Token not found. Please add HF_TOKEN secret.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformer is a PyTorch Framework used for training and inferencein the context of text, computer vision, audio, video, and multimodal model\n",
        "\n",
        "#Using Transfomers, every model is implemented from only three main classes (configuration, model, and preprocessor) and can be quickly used for inference or training with Pipeline or Trainer.\n",
        "\n",
        "#Main features:\n",
        " #Pipeline: inference class for tasks like text generation, image segmentation, automatic speech recognition, document question answering, and more.\n",
        "  #Useful for inference\n",
        " #Trainer: it supports features such as mixed precision, torch.compile, and FlashAttention for training and distributed training for PyTorch models.\n",
        "  #Useful for fine-tuning\n",
        " #Generate: Fast text generation with large language models (LLMs) and vision language models (VLMs), including support for streaming and multiple decoding strategies.\n",
        "\n",
        "import transformers"
      ],
      "metadata": {
        "id": "m6cQ8xKqwhOx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imporant Ideas:\n",
        "\n",
        "**Pretrained models**\n",
        "- The AutoClass API automatically infers the appropriate architecture for each task and machine learning framework based on the name or path to the pretrained weights and configuration file. This very useful in the context of loading \"models and preprocessors\"\n",
        "\n",
        "- from_pretrained(): load the weights and configuration file from the Hub into the model and preprocessor class.\n",
        "\n",
        "- parameter 1: device_map=\"auto\" automatically allocates the model weights to your fastest device first (CPU or GPU)\n",
        "\n",
        "- parameter 2:  dtype=\"auto\" directly initializes the model weights in the data type theyâ€™re stored in, which can help avoid loading the weights twice (PyTorch loads weights in torch.float32 by default).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T8wQkce1VXO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "#Answer:\n",
        "  #The warning (torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work) is just informing me that some optimizations aren't available without Triton.\n",
        "\n",
        "  #Triton: Triton: it is a library that helps accelerate certain types of matrix operations, commonly used in machine learning tasks. It's designed to optimize performance on GPUs (especially NVIDIA GPUs)...\n",
        "  #...by implementing custom kernel operations.\n",
        "\n",
        "  #I have 2 options:\n",
        "    #Ignore it if everything else is working fine.\n",
        "    #Install Triton if I want to take advantage of GPU optimizations."
      ],
      "metadata": {
        "id": "dMttPxtmnHiM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "#Model used by default: distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
        "  #This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. This model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).\n",
        "  #Developed by: Hugging Face\n",
        "  #Model Type: Text Classification\n",
        "  #Language(s): English"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dgcKHJfnhhx",
        "outputId": "35dff769-cf52-4726-8663-dbeffaa32752"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(\"I've been waiting for this kind of opportunity my whole life.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCnvHk-soVwz",
        "outputId": "6925de51-e081-4600-875c-1fdb56765b33"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9918659329414368}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(\n",
        "    [\"I do not like this movie\", \"Almonds are good, but I am alergic to them\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltWxESjNneSI",
        "outputId": "1d328083-c32a-48e1-c455-3eb540bb45e4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9869100451469421},\n",
              " {'label': 'POSITIVE', 'score': 0.9916054010391235}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}